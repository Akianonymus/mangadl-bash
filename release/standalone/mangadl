#!/usr/bin/env bash
SELF_SOURCE="true"
ALL_SOURCES=("fanfox" "gmanga" "manga4life" "mangafox" "mangakakalot" "manganelo" "readmanhwa" )
_basename(){
declare tmp
tmp=${1%"${1##*[!/]}"}
tmp=${tmp##*/}
tmp=${tmp%"${2/"$tmp"/}"}
printf '%s\n' "${tmp:-/}"
}
_bytes_to_human(){
declare b=${1:-0} d='' s=0 S=(Bytes {K,M,G,T,P,E,Y,Z}B)
while ((b>1024));do
d="$(printf ".%02d" $((b%1024*100/1024)))"
b=$((b/1024))&&((s++))
done
printf "%s\n" "$b$d ${S[$s]}"
}
_check_bash_version(){
{ ! [[ ${BASH_VERSINFO:-0} -ge 4 ]]&&printf "Bash version lower than 4.x not supported.\n"&&exit 1;}||:
}
_check_debug(){
_print_center_quiet(){ { [[ $# == 3 ]]&&printf "%s\n" "$2";}||{ printf "%s%s\n" "$2" "$3";};}
if [[ -n $DEBUG ]];then
set -x&&PS4='-> '
_print_center(){ { [[ $# == 3 ]]&&printf "%s\n" "$2";}||{ printf "%s%s\n" "$2" "$3";};}
_clear_line(){ :;}&&_newline(){ :;}
else
if [[ -z $QUIET ]];then
if _support_ansi_escapes;then
shopt -s checkwinsize&&(:&&:)
if [[ $COLUMNS -lt 45 ]];then
_print_center(){ { [[ $# == 3 ]]&&printf "%s\n" "[ $2 ]";}||{ printf "%s\n" "[ $2$3 ]";};}
else
trap 'shopt -s checkwinsize; (:;:)' SIGWINCH
fi
CURL_PROGRESS="-#" EXTRA_LOG="_print_center" CURL_PROGRESS_EXTRA="-#"
export CURL_PROGRESS EXTRA_LOG CURL_PROGRESS_EXTRA
else
_print_center(){ { [[ $# == 3 ]]&&printf "%s\n" "[ $2 ]";}||{ printf "%s\n" "[ $2$3 ]";};}
_clear_line(){ :;}
fi
_newline(){ printf "%b" "$1";}
else
_print_center(){ :;}&&_clear_line(){ :;}&&_newline(){ :;}
fi
set +x
fi
}
_check_internet(){
"${EXTRA_LOG:-:}" "justify" "Checking Internet Connection.." "-"
if ! _timeout 10 curl -Is google.com;then
_clear_line 1
"${QUIET:-_print_center}" "justify" "Error: Internet connection" " not available." "="
exit 1
fi
_clear_line 1
}
_check_and_create_range(){
declare mode="${1:?}" tmp&&shift
case "$mode" in
relative)for range in "$@"
do
unset start end _start _end
if [[ $range =~ ^([0-9.]+)-([0-9.]+|last)+$ ]];then
_start="${range/-*/}" _end="${range/*-/}"
[[ $_end == last ]]&&_end="${#PAGES[@]}"
if [[ $_start -gt $_end ]];then
_start="$_end"
_end="${range/-*/}"
elif [[ $_start -eq $_end ]];then
[[ $_start -lt 1 ]]&&_start=1
[[ $_start -gt ${#PAGES[@]} ]]&&_start="${#PAGES[@]}"
start="${PAGES[$((_start-1))]}"
[[ -z $start ]]&&printf "%s\n" "Error: invalid chapter ( $start )."&&return 1
RANGE+=("$start")
continue
fi
[[ $_start -lt 1 ]]&&_start=1
[[ $_end -gt ${#PAGES[@]} ]]&&_end="${#PAGES[@]}"
[[ $_start == "$_end" ]]&&unset _end
start="${PAGES[$((_start-1))]}"
[[ -z $start ]]&&printf "%s\n" "Error: invalid chapter ( $start )."&&return 1
[[ -n $_end ]]&&end="${PAGES[$((_end-1))]}"
RANGE+=("$start${end:+-$end}")
elif [[ $range =~ ^([0-9.]+|last)+$ ]];then
{ [[ $range == last ]]&&_start="${#PAGES[@]}";}||{
[[ $range -lt 1 ]]&&_start=1
[[ $range -gt ${#PAGES[@]} ]]&&_start="${#PAGES[@]}"
}
_start="${_start:-$range}"
start="${PAGES[$((_start-1))]}"
[[ -z $start ]]&&printf "%s\n" "Error: invalid chapter ( $start )."&&return 1
RANGE+=("$start")
else
printf "%s\n" "Error: Invalid range ( $range )."&&return 1
fi
done
;;
absolute)\
_tmp=" ${PAGES[*]} "
for range in "$@";do
unset _start _end
if [[ $range =~ ^([0-9.]+)-([0-9.]+|last)+$ ]];then
_start="${range//-*/}" _end="${range//*-/}"
[[ $_end == last ]]&&: "${#PAGES[@]}"&&_end="${PAGES[$((_-1))]}"
[[ -n ${_tmp//* $_start */} ]]&&printf "%s\n" "Error: invalid chapter ( $_start )."&&return 1
[[ -n ${_tmp//* $_end */} ]]&&printf "%s\n" "Error: invalid chapter ( $_end )."&&return 1
if [[ $_start > $_end ]];then
_start="$_end"
_end="${range/-*/}"
elif [[ $_start == "$_end" ]];then
RANGE+=("$_start")
continue
fi
RANGE+=("$_start${_end:+-$_end}")
elif [[ $range =~ ^([0-9.]+|last)+$ ]];then
{ [[ $range == last ]]&&_start="${#PAGES[@]}";}||_start="$range"
[[ -n ${_tmp//* $_start */} ]]&&printf "%s\n" "Error: invalid chapter ( $_start )."&&return 1
RANGE+=("$start")
else
printf "%s\n" "Error: Invalid range ( $range )."&&return 1
fi
done
esac
return 0
}
_clear_line(){
printf "\033[%sA\033[2K" "$1"
}
_count(){
mapfile -tn 0 lines
printf '%s\n' "${#lines[@]}"
}
_dirname(){
declare tmp=${1:-.}
[[ $tmp != *[!/]* ]]&&{ printf '/\n'&&return;}
tmp="${tmp%%"${tmp##*[!/]}"}"
[[ $tmp != */* ]]&&{ printf '.\n'&&return;}
tmp=${tmp%/*}&&tmp="${tmp%%"${tmp##*[!/]}"}"
printf '%s\n' "${tmp:-/}"
}
_display_time(){
declare T="$1"
declare DAY="$((T/60/60/24))" HR="$((T/60/60%24))" MIN="$((T/60%60))" SEC="$((T%60))"
[[ $DAY -gt 0 ]]&&printf '%d days ' "$DAY"
[[ $HR -gt 0 ]]&&printf '%d hrs ' "$HR"
[[ $MIN -gt 0 ]]&&printf '%d minute(s) ' "$MIN"
[[ $DAY -gt 0 || $HR -gt 0 || $MIN -gt 0 ]]&&printf 'and '
printf '%d seconds\n' "$SEC"
}
_get_latest_sha(){
declare LATEST_SHA
case "${1:-$TYPE}" in
branch)\
LATEST_SHA="$(: "$(curl --compressed -s https://github.com/"${3:-$REPO}"/commits/"${2:-$TYPE_VALUE}".atom -r 0-2000)"
: "$(printf "%s\n" "$_"|grep -o 'Commit\/.*<' -m1||:)"&&: "${_##*\/}"&&printf "%s\n" "${_%%<*}")"
;;
release)\
LATEST_SHA="$(: "$(curl -L --compressed -s https://github.com/"${3:-$REPO}"/releases/"${2:-$TYPE_VALUE}")"
: "$(printf "%s\n" "$_"|grep '="/'"${3:-$REPO}""/commit" -m1||:)"&&: "${_##*commit\/}"&&printf "%s\n" "${_%%\"*}")"
esac
printf "%b" "${LATEST_SHA:+$LATEST_SHA\n}"
}
_head(){
mapfile -tn "$1" line
printf '%s\n' "${line[@]}"
}
_json_value(){
declare num _tmp no_of_lines
{ [[ $2 -gt 0 ]]&&no_of_lines="$2";}||:
{ [[ $3 -gt 0 ]]&&num="$3";}||{ [[ $3 != all ]]&&num=1;}
_tmp="$(grep -o "\"$1\"\:.*" ${no_of_lines:+-m} $no_of_lines)"||return 1
printf "%s\n" "$_tmp"|sed -e 's/.*"'"$1""\"://" -e 's/[",]*$//' -e 's/["]*$//' -e 's/[,]*$//' -e "s/^ //" -e 's/^"//' -n -e "$num"p||:
}
_name(){
printf "%s\n" "${1%.*}"
}
_print_center(){
[[ $# -lt 3 ]]&&printf "%s: Missing arguments\n" "${FUNCNAME[0]}"&&return 1
declare -i TERM_COLS="$COLUMNS"
declare type="$1" filler
case "$type" in
normal)declare out="$2"&&symbol="$3";;
justify)if
[[ $# == 3 ]]
then
declare input1="$2" symbol="$3" TO_PRINT out
TO_PRINT="$((TERM_COLS-5))"
{ [[ ${#input1} -gt $TO_PRINT ]]&&out="[ ${input1:0:TO_PRINT}..]";}||{ out="[ $input1 ]";}
else
declare input1="$2" input2="$3" symbol="$4" TO_PRINT temp out
TO_PRINT="$((TERM_COLS*47/100))"
{ [[ ${#input1} -gt $TO_PRINT ]]&&temp+=" ${input1:0:TO_PRINT}..";}||{ temp+=" $input1";}
TO_PRINT="$((TERM_COLS*46/100))"
{ [[ ${#input2} -gt $TO_PRINT ]]&&temp+="${input2:0:TO_PRINT}.. ";}||{ temp+="$input2 ";}
out="[$temp]"
fi
;;
*)return 1
esac
declare -i str_len=${#out}
[[ $str_len -ge $((TERM_COLS-1)) ]]&&{
printf "%s\n" "$out"&&return 0
}
declare -i filler_len="$(((TERM_COLS-str_len)/2))"
[[ $# -ge 2 ]]&&ch="${symbol:0:1}"||ch=" "
for ((i=0; i<filler_len; i++));do
filler="$filler$ch"
done
printf "%s%s%s" "$filler" "$out" "$filler"
[[ $(((TERM_COLS-str_len)%2)) -ne 0 ]]&&printf "%s" "$ch"
printf "\n"
return 0
}
_support_ansi_escapes(){
{ [[ -t 2 && -n $TERM && $TERM =~ (xterm|rxvt|urxvt|linux|vt) ]]&&return 0;}||return 1
}
_regex(){
[[ $1 =~ $2 ]]&&printf '%s\n' "${BASH_REMATCH[${3:-0}]}"
}
_remove_array_duplicates(){
[[ $# == 0 ]]&&printf "%s: Missing arguments\n" "${FUNCNAME[0]}"&&return 1
declare -A Aseen
Aunique=()
for i in "$@";do
{ [[ -z $i || ${Aseen[$i]} ]];}&&continue
Aunique+=("$i")&&Aseen[$i]=x
done
printf '%s\n' "${Aunique[@]}"
}
_reverse(){
for ((i="${#*}"; i>0; i--));do
printf "%s\n" "${!i}"
done
}
_tail(){
mapfile -tn 0 line
printf '%s\n' "${line[@]: -$1}"
}
_timeout(){
declare timeout="${1:?Error: Specify Timeout}"&&shift
{
"$@"&
child="$!"
trap -- "" TERM
{
sleep "$timeout"
kill -9 "$child"
}&
wait "$child"
} 2>|/dev/null 1>&2
}
_update_config(){
[[ $# -lt 3 ]]&&printf "Missing arguments\n"&&return 1
declare value_name="$1" value="$2" config_path="$3"
! [ -f "$config_path" ]&&: >|"$config_path"
chmod u+w "$config_path"
printf "%s\n%s\n" "$(grep -v -e "^$" -e "^$value_name=" "$config_path"||:)" \
"$value_name=\"$value\"" >|"$config_path"
chmod u-w+r "$config_path"
}
_upload(){
[[ $# == 0 ]]&&printf "%s: Missing arguments\n" "${FUNCNAME[0]}"&&return 1
declare input="$1" json id link
if ! [[ -f $input ]];then
printf "Given file ( %s ) doesn't exist\n" "$input"
return 1
fi
json="$(curl "-#" -F 'file=@'"$input" "https://pixeldrain.com/api/file")"||return 1
id="$(: "${json/*id*:\"/}"&&printf "%s\n" "${_/\"\}/}")"
link="https://pixeldrain.com/api/file/$id?download"
printf "%s\n" "$link"
}
_url_encode(){
declare LC_ALL=C
for ((i=0; i<${#1}; i++));do
: "${1:i:1}"
case "$_" in
[a-zA-Z0-9.~_-$2])printf '%s' "$_"
;;
*)printf '%%%02X' "'$_"
esac
done
printf '\n'
}
_download_chapter(){
declare page="$1" parallel="$2"
if [[ -f "$page/$page"_chapter && $(_tail 1 <"$page/$page"_chapter) =~ 200 ]];then
[[ -n $parallel ]]&&printf "1\n"||return 0
else
if _download_chapter_"$SOURCE" >|"$page/$page"_chapter;then
[[ -n $parallel ]]&&printf "1\n"||return 0
else
[[ -n $parallel ]]&&printf "2\n" 1>&2||return 1
fi
fi
}
_fetch_manga_chapters(){
declare SUCCESS_STATUS=0 ERROR_STATUS=0 pid
if [[ -n $PARALLEL_DOWNLOAD ]];then
{ [[ $NO_OF_PARALLEL_JOBS -gt ${#PAGES[@]} ]]&&NO_OF_PARALLEL_JOBS="${#PAGES[@]}";}||:
export -f _download_chapter _download_chapter_"$SOURCE"
printf "%s\n" "${PAGES[@]}"|xargs -n1 -P"$NO_OF_PARALLEL_JOBS" -i bash -c \
'_download_chapter "{}" true' 1>"$TMPFILE".success 2>"$TMPFILE".error&
pid="$!"
until [[ -f "$TMPFILE".success || -f "$TMPFILE".error ]];do sleep 0.5;done
_newline "\n"
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
SUCCESS_STATUS="$(_count <"$TMPFILE".success)"
ERROR_STATUS="$(_count <"$TMPFILE".error)"
sleep 1
if [[ $TOTAL_STATUS != "$((SUCCESS_STATUS+ERROR_STATUS))" ]];then
_clear_line 1
_print_center "justify" "$SUCCESS_STATUS success" " | $ERROR_STATUS failed" "="
fi
TOTAL_STATUS="$((SUCCESS_STATUS+ERROR_STATUS))"
done
rm -f "$TMPFILE".success "$TMPFILE".error
else
_newline "\n"
for page in "${PAGES[@]}";do
if _download_chapter "$page";then
SUCCESS_STATUS="$((SUCCESS_STATUS+1))"
else
ERROR_STATUS="$((ERROR_STATUS+1))"
fi
_clear_line 1 1>&2
_print_center "justify" "$SUCCESS_STATUS success" " | $ERROR_STATUS failed" "=" 1>&2
done
fi
_clear_line 1
return 0
}
_download_images(){
declare SUCCESS_STATUS=0 ERROR_STATUS=0 pid
if [[ -n $PARALLEL_DOWNLOAD ]];then
{ [[ $NO_OF_PARALLEL_JOBS -gt ${#PAGES[@]} ]]&&NO_OF_PARALLEL_JOBS="${#PAGES[@]}";}||:
printf "%s\n" "${PAGES[@]}"|xargs -n1 -P"$NO_OF_PARALLEL_JOBS" -i \
wget -P "{}" --referer="$REFERER" -c -i "{}/{}"_images &>"$TMPFILE".log&
pid="$!"
until [[ -f "$TMPFILE".log ]];do sleep 0.5;done
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
SUCCESS_STATUS="$(grep -ic 'retrieved\|saved' "$TMPFILE".log)"
ERROR_STATUS="$(grep -ic 'ERROR 404' "$TMPFILE".log)"
sleep 2
if [[ $TOTAL_STATUS != "$((SUCCESS_STATUS+ERROR_STATUS))" ]];then
_clear_line 1
_print_center "justify" "$SUCCESS_STATUS success" " | $ERROR_STATUS failed" "="
fi
TOTAL_STATUS="$((SUCCESS_STATUS+ERROR_STATUS))"
done
rm -f "$TMPFILE".log
else
for page in "${PAGES[@]}";do
log="$(wget --referer="$REFERER" -P "$page" -c -i "$page/$page"_images 2>&1)"
SUCCESS_STATUS="$(($(grep -ic 'retrieved\|saved' <<<"$log")+SUCCESS_STATUS))"
ERROR_STATUS="$(($(grep -ic 'ERROR 404' <<<"$log")+ERROR_STATUS))"
_clear_line 1 1>&2
_print_center "justify" "$SUCCESS_STATUS success" " | $ERROR_STATUS failed" "=" 1>&2
done
fi
_clear_line 1
shopt -s extglob
mapfile -t IMAGES <<<"$(_tmp="$(printf "%s/*+(jpg|png|webp)\n" "${PAGES[@]}")"&&printf "%b\n" $_tmp)"
TOTAL_IMAGES_SIZE="$(: "$(wc -c "${IMAGES[@]}"|_tail 1|grep -Eo '[0-9]'+)"
printf "%s\n" "$(_bytes_to_human "$_")")"
export TOTAL_IMAGES_SIZE IMAGES
shopt -u extglob
return 0
}
_search_manga_gmanga(){
declare input="$1" num_of_search="$2" post_data search_json
post_data="$(jq -n --arg q "$input" '{query: $q, includes:["Manga"]}')"
search_json="$(curl -# --compressed https://gmanga.org/api/quick_search \
-d "$post_data" \
-e "gmanga.org" -H "Content-Type: application/json")"
[[ $search_json =~ \"data\":*\[\] ]]&&return 1
declare manga_info&&declare -g NAMES TOTAL_SEARCHES SLUGS
mapfile -t NAMES <<<"$(jq '.[].data[].title' -r <<<"$search_json")"
mapfile -t manga_info <<<"$(jq '.[].data[].id,.[].data[].latest_chapter,.[].data[].story_status' -r <<<"$search_json")"
TOTAL_SEARCHES="${#NAMES[@]}"
i=1
for name in "${NAMES[@]}";do
num="$((i++))"
[[ $num -gt $num_of_search ]]&&break
num_id="$((num-1))" id="${manga_info[$num_id]//null/Unknown}" SLUGS+=("$id")
num_latest="$((num_id+TOTAL_SEARCHES))" latest="${manga_info[$num_latest]//null/Unknown}"
num_status="$((num_latest+TOTAL_SEARCHES))" status="${manga_info[$num_status]//null/Unknown}"&&case "$status" in
2)status="Ongoing";;
3)status="Completed";;
*)status="Unknown"
esac
OPTION_NAMES+=("$num. $name
   URL: https://gmanga.org/mangas/$id
   Latest: $latest
   Status: $status")
done
export TOTAL_SEARCHES NAMES SLUGS OPTION_NAMES
}
_set_manga_variables_gmanga(){
declare option="$1"
SLUG="${SLUGS[$((option-1))]}"
NAME="${NAMES[$((option-1))]}"
export SLUG NAME
}
_fetch_manga_details_gmanga(){
declare slug="${1:-$SLUG}" fetch_name="${2:-}" json
slug="$(_basename "$slug")"
json="$(curl -L -# --compressed "https://gmanga.org/api/mangas/$slug")"
_clear_line 1
case "$json" in
*'"mangaData":null'*|*'"error":"Internal Server Error"'*)return 1
esac
[[ -n $fetch_name ]]&&NAME="$(jq '.mangaData.title' -r <<<"$json")"
_print_center "justify" "Retrieving manga" " chapters.." "-"
declare enc_json enc_data dec_data dec_json key iv
enc_json="$(curl -# -L --compressed "https://gmanga.org/api/mangas/$slug/releases")"&&_clear_line 1
enc_data="$(: "$(jq -re '.data' <<<"$enc_json")"&&printf '%s' "${_//|/$'\n'}")"||return 1
dec_data="$(sed -n 1p <<<"$enc_data")"
key="$(sed -n 4p <<<"$enc_data")"&&key="$(openssl sha256 <<<"$key")"&&key="${key//* /}"
iv="$(sed -n 3p <<<"$enc_data")"&&iv="$(openssl base64 -A -d <<<"$iv"|od -A n -t x1)"&&iv="${iv//$'\n'/}"&&iv="${iv// /}"
dec_json="$(openssl aes-256-cbc -d -K "$key" -iv "$iv" -in <(openssl base64 -A -d <<<"$dec_data") -out -)" 2>|/dev/null||return 1
mapfile -t PAGES <<<"$(jq '.rows[2].rows[][1]' -re <<<"$dec_json"|sort -V)"||return 1
_clear_line 1
export SLUG="$slug" PAGES REFERER="gmanga.org"
}
_download_chapter_gmanga(){
curl -s -L --compressed "https://gmanga.org/mangas/$SLUG/$NAME/${page:-?}/" -w "\n%{http_code}\n"
}
_count_images_gmanga(){
TOTAL_IMAGES="$(: "$(for page in "${PAGES[@]}";do
{
json="$(grep 'class="js-react-on-rails-component"' "$page/$page"_chapter|grep -oE '\{.*\}')"||:
extra_string="_webp"
images="$(jq '.readerDataAction.readerData.release.webp_pages[]' -re <<<"$json")"||{
images="$(jq '.readerDataAction.readerData.release.pages[]' -re <<<"$json")"
extra_string=""
}
release="$(jq '.readerDataAction.readerData.release.storage_key' -r <<<"$json")"
printf '%s\n' "$images"|sed "s|^|https://media.$REFERER/uploads/releases/$release/mq$extra_string/|g" >|"$page/$page"_images
_count <"$page/$page"_images
}&
done)"&&printf "%s\n" "$((${_//$'\n'/ + }))")"
export TOTAL_IMAGES
}
_search_manga_manga4life(){
declare file_path="$HOME/.manga4life.list"
fetch="true"
if [[ -f $file_path ]]&&grep -q "vm.Directory" "$file_path"&&[[ $(($(date -r "$file_path" +%s)+43200)) -gt $(printf '%(%s)T\n' "-1") ]];then
fetch="false"
fi
[[ $fetch == "true" ]]&&{
_print_center "justify" "Fetching manga4life manga list.." "-"
chmod u+w -f "$file_path"
curl -sL --compressed "https://manga4life.com/search/" -o "$file_path"||{
printf "%s\n" "Error: Couldn't fetch manga list." 1>&2&&return 1
}
_clear_line 1
manga_list="$(grep -o "vm.Directory = .*" "$file_path"|sed -e \
"s/\"i\"/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"i\"/g" \
-e "s/\"o\"/\n\"o\"/g" \
-e "s/\"s\"/\n\"s\"/g" \
-e "s/\"ss\"/\n\"ss\"/g" \
-e "s/\"ps\"/\n\"ps\"/g" \
-e "s/\"t\"/\n\"t\"/g" \
-e "s/\"v\"/\n\"v\"/g" \
-e "s/\"vm\"/\n\"vm\"/g" \
-e "s/\"y\"/\n\"y\"/g" \
-e "s/\"a\"/\n\"a\"/g" \
-e "s/\"al\"/\n\"al\"/g" \
-e "s/\"lt\"/\n\"lt\"/g" \
-e "s/\"l\"/\n\"l\"/g" \
-e "s/\"ls\"/\n\"ls\"/g" \
-e "s/\"g\"/\n\"g\"/g" \
-e "s/\"h\"/\n\"h\"/g")"
printf "%s\n" "$manga_list" >|"$file_path"
}
declare input="$1" num_of_search="$2"
if search_results="$(grep -i -A 16 "\"s\".*$input" "$file_path" ${num_of_search:+-m $num_of_search})";then
mapfile -t slugs <<<"$(grep -i -B 1 "\"s\".*$input" "$file_path" ${num_of_search:+-m $num_of_search}|grep '"i"'|sed -e "s/^\"i\"\:\"//g" -e "s/\",$//g")"
mapfile -t names <<<"$(grep '"s"' <<<"$search_results"|sed -e "s/^\"s\"\:\"//g" -e "s/\",$//g")"
mapfile -t status <<<"$(grep '"ss"' <<<"$search_results"|sed -e "s/^\"ss\"\:\"//g" -e "s/\",$//g")"
mapfile -t latest <<<"$(grep '"l"' <<<"$search_results"|sed -e "s/^\"l\"\:\"//g" -e "s/\",$//g")"
mapfile -t updated <<<"$(grep '"ls"' <<<"$search_results"|sed -e "s/^\"ls\"\:\"//g" -e "s/T.*//g")"
i=1
while read -r -u 4 name&&read -r -u 5 _latest&&read -r -u 6 _update&&read -r -u 7 slug&&read -r -u 8 _status;do
num="$((i++))"
OPTION_NAMES+=("$num. $name
   URL: https://manga4life.com/manga/$slug
   Status: $_status
   Latest: Chapter $(: "${_latest#[0-9]}"&&printf "%s\n" "${_%0}"|grep -Eo '[1-9]+[0-9]+$'||printf "%s\n" "not known"), $_update")
done 4<<<"$(printf "%s\n" "${names[@]}")" 5<<<"$(printf "%s\n" "${latest[@]}")" 6<<<"$(printf "%s\n" "${updated[@]}")" 7<<<"$(printf "%s\n" "${slugs[@]}")" 8<<<"$(printf "%s\n" "${status[@]}")"
else
return 1
fi
TOTAL_SEARCHES="${#slugs[@]}"
export TOTAL_SEARCHES OPTION_NAMES
}
_set_manga_variables_manga4life(){
declare option="$1"
SLUG="${slugs[$((option-1))]}"
NAME="${names[$((option-1))]}"
export SLUG NAME
}
_fetch_manga_details_manga4life(){
declare slug fetch_name="${2:-}" HTML url
slug="$(_basename "${1:-$SLUG}")"&&SLUG="$slug"
url="https://manga4life.com/manga/$slug"
HTML="$(curl -# --compressed "$url" -w "\n%{http_code}\n")"
_clear_line 1
[[ $HTML == *"We're sorry, the page you"* ]]&&return 1
if [[ -n $fetch_name ]];then
NAME="$(: "$(grep -o 'h1>.*</h1' <<<"$HTML")"&&: "${_//*h1>/}"&&printf "%s\n" "${_//<\/h1*/}")"
fi
mapfile -t _PAGES <<<"$(grep 'vm.Chapters = ' <<<"$HTML"|grep -Eo '[0-9]+{6}')"
mapfile -t PAGES <<<"$(printf "%s\n" "${_PAGES[@]}"|sed -E -e "s/^[0-9]//g" -e "s/[1-9]$/.&/g" -e "s/0$//g" -e "s/^0*//g"|grep -Eo '[0-9.]+'|sed -e "s/^\./0\./g")"
while read -r -u 4 long_page&&read -r -u 5 short_page;do
export "INDEX_${short_page//./_}=${long_page:0:1}"
done 4<<<"$(printf "%s\n" "${_PAGES[@]}")" 5<<<"$(printf "%s\n" "${PAGES[@]}")"
mapfile -t PAGES <<<"$(_reverse "${PAGES[@]}")"
mapfile -t PAGES <<<"$(_remove_array_duplicates "${PAGES[@]}")"
export SLUG PAGES REFERER="manga4life.com"
}
_download_chapter_manga4life(){
declare index="INDEX_${page//./_}"
curl -s -L --compressed "https://manga4life.com/read-online/$SLUG-chapter-$page-index-${!index}.html" -w "\n%{http_code}\n"
}
_count_images_manga4life(){
TOTAL_IMAGES="$(: "$(for page in "${PAGES[@]}";do
{
_tmp="$(grep -E 'vm.CurChapter = |vm.CurPathName = ' "$page/$page"_chapter)"
server="$(: "${_tmp##*vm.CurPathName = \"}"&&printf "%s\n" "${_%%\";*}")"
total_images="$(: "${_tmp##*\"Page\"\:\"}"&&printf "%s\n" "${_%%\"*}")"
dir="$(: "${_tmp##*\"Directory\"\:\"}"&&printf "%s\n" "${_%%\"*}")"
_tmp="000$page"
if [[ $page == *"."* ]];then
chap="${_tmp: -6}"
else
chap="${_tmp: -4}"
fi
for ((i=1; i<=total_images; i++));do
: "00$i"&&image="${_: -3}"
printf "%s\n" "https://$server/manga/$SLUG${dir:+/$dir}/$chap-$image.png"
done >|"$page/$page"_images
printf "%s\n" "$total_images"
}&
done)"&&printf "%s\n" "$((${_//$'\n'/ + }))")"
export TOTAL_IMAGES
}
_search_manga_mangafox(){
declare input="${1// /+}" num_of_search="$2"
SEARCH_HTML="$(curl -# --compressed http://m.fanfox.net/search?k="$(_url_encode "$input" +)")"
_clear_line 1
[[ $SEARCH_HTML == *"No Manga Series."* ]]&&return 1
SEARCH_HTML="$(grep --no-group-separator "post-one clearfix" -A 8 ${num_of_search:+-m $num_of_search} <<<"$SEARCH_HTML")"
mapfile -t names <<<"$(grep '"title"' <<<"$SEARCH_HTML"|grep -o '>.*<'|sed "s/\(^>\|<$\)//g")"
mapfile -t urls <<<"$(grep 'href="' <<<"$SEARCH_HTML"|grep -Eo "(http|https)://[a-zA-Z0-9./?=_%:-]*")"
mapfile -t status <<<"$(: "$(grep -Eo "Status:.*(Completed|Ongoing)" <<<"$SEARCH_HTML")"&&: "${_//Status: /}"&&printf "%s\n" "${_//Status:/}")"
mapfile -t genre <<<"$(: "$(grep --no-group-separator 'Status:' -B 1 <<<"$SEARCH_HTML"|grep -v 'Status:')"&&: "${_//<p>/}"&&printf "%s\n" "${_//<\/p>/}")"
i=1
while read -r -u 4 name&&read -r -u 5 _status&&read -r -u 6 _genre&&read -r -u 7 url;do
num="$((i++))"
OPTION_NAMES+=("$num. $name
   URL: $url
   Status: $_status
   Genre: $_genre")
done 4<<<"$(printf "%s\n" "${names[@]}")" 5<<<"$(printf "%s\n" "${status[@]}")" 6<<<"$(printf "%s\n" "${genre[@]}")" 7<<<"$(printf "%s\n" "${urls[@]}")"
TOTAL_SEARCHES="${#names[@]}"
export TOTAL_SEARCHES OPTION_NAMES
}
_set_manga_variables_mangafox(){
declare option="$1"
SLUG="$(_basename "${urls[$((option-1))]}")"
NAME="${names[$((option-1))]}"
export SLUG NAME
}
_fetch_manga_details_mangafox(){
declare slug fetch_name="${2:-}" HTML&&unset VOLUMES
slug="$(_basename "${1:-$SLUG}")"
HTML="$(curl -# --compressed -L "http://m.fanfox.net/manga/$slug" -w "\n%{http_code}\n")"
_clear_line 1
[[ $HTML == *"The page you were looking for doesn"* ]]&&return 1
if [[ -n $fetch_name ]];then
NAME="$(: "$(grep 'title.*title' <<<"$HTML")"&&: "${_/<title>/}"&&printf "%s\n" "${_/<\/title>/}")"
fi
! [[ $HTML == *"Volume Not Available"* ]]&&export VOLUMES="true"
mapfile -t PAGES <<<"$(: "$(grep -oE "$slug.*c[0-9.]+[0-9]/1.html" <<<"$HTML")"&&: "${_//\/1.html/}"&&: "${_//$slug\//}"&&printf "%s\n" "${_//${VOLUMES:+\/}c0/${VOLUMES:+_}}"|sed 1d)"
mapfile -t PAGES <<<"$(_reverse "${PAGES[@]}")"
export PAGES REFERER="fanfox.net"
}
_download_chapter_mangafox(){
declare vol _page
[[ -n $VOLUMES ]]&&vol="${page//_*/}" _page="${page//*_/}"
curl -s "http://m.fanfox.net/roll_manga/$SLUG/${vol:+$vol/}c0${_page:-$page}/1.html" -w "\n%{http_code}\n"
}
_count_images_mangafox(){
TOTAL_IMAGES="$(: "$(for page in "${PAGES[@]}";do
{
grep -Eo "(http|https)://[a-zA-Z0-9./?=_%:-]*(jpg|png)" "$page/$page"_chapter >|"$page/$page"_images
_count <"$page/$page"_images
}&
done)"&&printf "%s\n" "$((${_//$'\n'/ + }))")"
export TOTAL_IMAGES
}
_search_manga_manganelo(){
declare input="${1// /_}" num_of_search="$2"
SEARCH_HTML="$(curl -# --compressed https://mangakakalot.com/search/story/"$input")"
_clear_line 1
[[ $SEARCH_HTML != *"story_name"* ]]&&return 1
SEARCH_HTML="$(grep "story_name" -A 12 ${num_of_search:+-m $num_of_search} <<<"$SEARCH_HTML")"
mapfile -t names <<<"$(grep "story_name" -A 1 <<<"$SEARCH_HTML"|grep -E "(http|https)://[a-zA-Z0-9./?=_%:-]*"|grep -o '>.*<'|sed "s/\(^>\|<$\)//g")"
mapfile -t urls <<<"$(grep "story_name" -A 1 <<<"$SEARCH_HTML"|grep -Eo "(http|https)://[a-zA-Z0-9./?=_%:-]*")"
mapfile -t latest <<<"$(: "$(grep "story_chapter" -A 1 <<<"$SEARCH_HTML"|grep -v '</a>\|story_chapter'|grep -o 'title=\".*\"')"&&printf "%s\n" "${_//title=/}")"
mapfile -t updated <<<"$(grep -o "Updated.*[0-9]" <<<"$SEARCH_HTML")"
i=1
while read -r -u 4 name&&read -r -u 5 _latest&&read -r -u 6 _update&&read -r -u 7 url;do
num="$((i++))"
OPTION_NAMES+=("$num. $name
   URL: $url
   Latest: $_latest
   $_update")
done 4<<<"$(printf "%s\n" "${names[@]}")" 5<<<"$(printf "%s\n" "${latest[@]}")" 6<<<"$(printf "%s\n" "${updated[@]}")" 7<<<"$(printf "%s\n" "${urls[@]}")"
TOTAL_SEARCHES="${#names[@]}"
export TOTAL_SEARCHES OPTION_NAMES
}
_set_manga_variables_manganelo(){
declare option="$1"
URL="${urls[$((option-1))]}"
NAME="${names[$((option-1))]}"
export URL NAME
}
_fetch_manga_details_manganelo(){
declare url="${1:-$URL}" fetch_name="${2:-}" HTML
HTML="$(curl -# --compressed -L "$url" -w "\n%{http_code}\n")"
_clear_line 1
[[ $HTML == *"Sorry, the page you have requested cannot be found"* ]]&&return 1
if [[ -n $fetch_name ]];then
NAME="$(: "$(grep -o 'h1>.*</h1' <<<"$HTML")"&&: "${_//*h1>/}"&&printf "%s\n" "${_//<\/h1*/}")"
fi
if [[ $url =~ mangakakalot ]];then
mapfile -t URL_PAGES <<<"$(grep -F 'div class="row"' -A 1 <<<"$HTML"|grep -Eo "(http|https)://mangakakalot[a-zA-Z0-9./?=_%:-]*")"
else
mapfile -t URL_PAGES <<<"$(grep -F chapter-name <<<"$HTML"|grep -Eo "(http|https)://[a-zA-Z0-9./?=_%:-]*")"
fi
mapfile -t URL_PAGES <<<"$(_reverse "${URL_PAGES[@]}")"
mapfile -t PAGES <<<"$(printf "%s\n" "${URL_PAGES[@]}"|sed "s/.*\///g"|grep -oE '[0-9.]+')"
CHAPTER_STRING="$(_basename "${URL_PAGES[0]//[0-9.]/}")"
MANGA_URL="${URL_PAGES[0]/$CHAPTER_STRING*/}"
export CHAPTER_STRING PAGES MANGA_URL REFERER="manganelo.com"
}
_download_chapter_manganelo(){
curl -s -L --compressed "$MANGA_URL$CHAPTER_STRING$page" -w "\n%{http_code}\n"
}
_count_images_manganelo(){
TOTAL_IMAGES="$(: "$(for page in "${PAGES[@]}";do
{
grep -Eo "(http|https)://[a-zA-Z0-9./?=_%:-]*chapter_[a-zA-Z0-9./?=_%:-]*.(jpg|png)" "$page/$page"_chapter >|"$page/$page"_images
_count <"$page/$page"_images
}&
done)"&&printf "%s\n" "$((${_//$'\n'/ + }))")"
export TOTAL_IMAGES
}
_search_manga_readmanhwa(){
declare input="$1" num_of_search="$2"
SEARCH_JSON="$(curl -# --compressed "https://readmanhwa.com/api/comics?nsfw=true&q=$(_url_encode "$input")&per_page=$num_of_search&sort=title" \
-H "X-NSFW: true" -H "Accept-Language: en")"
_clear_line 1
SEARCH_JSON="${SEARCH_JSON//\}\]\},\{/$'\n'}"
SEARCH_JSON="$(sed -e "s/thumb_url.*//g" <<<"$SEARCH_JSON")"
[[ $SEARCH_JSON =~ \"total\":0 ]]&&return 1
for i in title slug alternative_title description rewritten translated speechless uploaded_at pages favorites chapters_count status thumb_url;do
SEARCH_JSON="${SEARCH_JSON//\"$i\"/$'\n'\"$i\"}"
done
mapfile -t names <<<"$(_json_value title all all <<<"$SEARCH_JSON")"
mapfile -t slugs <<<"$(_json_value slug all all <<<"$SEARCH_JSON")"
mapfile -t latest <<<"$(_json_value uploaded_at all all <<<"$SEARCH_JSON")"
mapfile -t status <<<"$(_json_value status all all <<<"$SEARCH_JSON")"
i=1
while read -r -u 4 name&&read -r -u 5 _latest&&read -r -u 6 _status&&read -r -u 7 _slug;do
num="$((i++))"
OPTION_NAMES+=("$num. $name
   URL: https://readmanhwa.com/comics/$_slug
   Latest: $_latest
   Status: $_status")
done 4<<<"$(printf "%s\n" "${names[@]}")" 5<<<"$(printf "%s\n" "${latest[@]}")" 6<<<"$(printf "%s\n" "${status[@]}")" 7<<<"$(printf "%s\n" "${slugs[@]}")"
TOTAL_SEARCHES="${#names[@]}"
export OPTION_NAMES TOTAL_SEARCHES
}
_set_manga_variables_readmanhwa(){
declare option="$1"
SLUG="${slugs[$((option-1))]}"
NAME="${names[$((option-1))]}"
LATEST="${latest[$((option-1))]}"
export NAME LATEST
}
_fetch_manga_details_readmanhwa(){
declare slug last _pages
slug="$(_basename "${1:-$SLUG}")"
_print_center "justify" "Retrieving manga" " chapters.." "-"
mapfile -t _pages <<<"$(curl -# --compressed -H "X-NSFW: true" "https://readmanhwa.com/api/comics/$slug/chapters?nsfw=true"|grep -Eo "chapter-[0-9.]+"|grep -Eo "[0-9.]+")"
[[ -z ${_pages[*]} ]]&&return 1
last=${#_pages[@]}
for ((i=last-1; i>=0; i--));do
PAGES+=("${_pages[i]}")
done
for _ in {1..2};do _clear_line 1;done
export PAGES REFERER="readmanhwa.com"
}
_download_chapter_readmanhwa(){
curl -s "https://readmanhwa.com/api/comics/$SLUG/chapter-$page/images?nsfw=true" -H "X-NSFW: true" -w "\n%{http_code}\n"
}
_count_images_readmanhwa(){
TOTAL_IMAGES="$(: "$(for page in "${PAGES[@]}";do
{
json_images="$(<"$page/$page"_chapter)"
for i in source_url thumbnail_url;do
json_images="${json_images//\"$i\"/$'\n'\"$i\"}"
done
_json_value "source_url" all all <<<"${json_images//'\/'/\/}" >|"$page/$page"_images
_count <"$page/$page"_images
}&
done)"&&printf "%s\n" "$((${_//$'\n'/ + }))")"
export TOTAL_IMAGES
}
_usage(){
printf "%b" "
The script can be used to search and download mangas from various sources.\n

Supported sources: ${ALL_SOURCES[*]:-No sources available.}

Default source: ${SOURCE:-Not set}

Usage: ${0##*/} manga_name/manga_url [options.. ]\n
Options:\n
  -d | --directory - Custom workspace folder.\n
  -s | --source 'name of source' - Source where the input will be searched.\n
      To change default source, use mangadl -s default=sourcename\n
  -n | --num 'no of searches' - No. of searches to show, default is 10.\n
      To change default no of searches, use mangadl -n default='no of searches'\n
  -p | --parallel 'no of jobs'  - No. of parallel jobs to use.\n
  -r | --range - Custom range, can be given with this flag as argument, or if not given, then will be asked later in the script.\n
      e.g: -r '1 5-10 11 12-last last', this will download chapter number 1, 5 to 10 and 11. For more info, see README.\n
  -ra | --range-absolute - This is same as range flag except it uses the given range as the absolute number present on the respective website. For more info, see README.\n
  -c | --convert 'quality between 0 to 99' - Decrease quality of images by the given percentage using convert ( imagemagick ).\n
  -z | --zip - Create zip of downloaded images.\n
  --upload - Upload created zip on pixeldrain.com.\n
  --skip-internet-check - Like the flag says.\n
  --info - Show detailed info, only if script is installed system wide.\n
  -u | --update - Update the installed script in your system.\n
  --uninstall - Uninstall script, remove related files.\n
  -D | --debug - Display script command trace, use before all the flags to see maximum script trace.\n
  -h | --help - Display usage instructions.\n"
exit
}
_short_help(){
printf "No valid arguments provided, use -h/--help flag to see usage.\n"
exit
}
_auto_update(){
export REPO
(\
_REPO="$REPO"
command -v "$COMMAND_NAME" 1>/dev/null&&if [[ -n ${_REPO:+${COMMAND_NAME:+${INSTALL_PATH:+${TYPE:+$TYPE_VALUE}}}} ]];then
current_time="$(printf '%(%s)T\n' "-1")"
[[ $((LAST_UPDATE_TIME+AUTO_UPDATE_INTERVAL)) -lt $current_time ]]&&_update
_update_value LAST_UPDATE_TIME "$current_time"
fi) \
2>|/dev/null \
1>&2&
return 0
}
_update(){
declare job="${1:-update}"
[[ $GLOBAL_INSTALL == true ]]&&! [[ $(id -u) == 0 ]]&&printf "%s\n" "Error: Need root access to update."&&return 0
[[ $job =~ uninstall ]]&&job_string="--uninstall"
_print_center "justify" "Fetching $job script.." "-"
declare repo="${REPO:-Akianonymus/mangadl-bash}" type_value="${TYPE_VALUE:-latest}"
{ [[ ${TYPE:-} != branch ]]&&type_value="$(_get_latest_sha release "$type_value" "$repo")";}||:
if script="$(curl --compressed -Ls "https://raw.githubusercontent.com/$repo/$type_value/release/install")";then
_clear_line 1
printf "%s\n" "$script"|bash -s -- ${job_string:-} --skip-internet-check
current_time="$(printf '%(%s)T\n' "-1")"
_update_value LAST_UPDATE_TIME "$current_time"&
else
_clear_line 1
"${QUIET:-_print_center}" "justify" "Error: Cannot download $job script." "=" 1>&2
exit 1
fi
exit "$?"
}
_update_value(){
declare command_path="${INSTALL_PATH:?}/$COMMAND_NAME" \
value_name="${1:-}" value="${2:-}" script_without_value_and_shebang
script_without_value_and_shebang="$(grep -v "$value_name=\".*\".* # added values" "$command_path"|sed 1d)"
new_script="$(sed -n 1p "$command_path"
printf "%s\n" "$value_name=\"$value\" # added values"
printf "%s\n" "$script_without_value_and_shebang")"
chmod +w "$command_path"&&printf "%s\n" "$new_script" >|"$command_path"&&chmod -w "$command_path"
return 0
}
_version_info(){
if command -v "$COMMAND_NAME" 1>/dev/null&&[[ -n ${REPO:+${COMMAND_NAME:+${INSTALL_PATH:+${TYPE:+$TYPE_VALUE}}}} ]];then
for i in REPO INSTALL_PATH TYPE TYPE_VALUE LATEST_INSTALLED_SHA;do
printf "%s\n" "$i=\"${!i}\""
done|sed -e "s/=/: /g"
else
printf "%s\n" "mangadl-bash is not installed system wide."
fi
exit 0
}
_setup_arguments(){
unset DEBUG FOLDER SOURCE NO_OF_PARALLEL_JOBS PARALLEL_DOWNLOAD MAX_BACKGROUD_JOBS NUM_OF_SEARCH \
MODIFY_RANGE GIVEN_RANGE ABSOLUTE_GIVEN_RANGE DECREASE_QUALITY CONVERT CONVERT_DIR CREATE_ZIP UPLOAD_ZIP SKIP_INTERNET_CHECK INPUT_ARRAY
CONFIG="$HOME/.mangadl-bash.conf"
[[ -f $CONFIG ]]&&. "$CONFIG"
SOURCE="${SOURCE:-mangafox}"
_check_longoptions(){
[[ -z $2 ]]&&printf '%s: %s: option requires an argument\nTry '"%s -h/--help"' for more information.\n' "${0##*/}" "$1" "${0##*/}"&&exit 1
return 0
}
url_regex='(http|https)://[a-zA-Z0-9./?=_%:-]*'
source_regex='(http|https)://.*('$(printf "%s|" "${ALL_SOURCES[@]}")examplemanga')[a-zA-Z0-9./?=_%:-]'
while [[ $# -gt 0 ]];do
case "$1" in
-h|--help)_usage;;
-D|--debug)DEBUG="true"&&export DEBUG;;
-u|--update)_check_debug&&_update;;
--uninstall)_check_debug&&_update uninstall;;
--info)_version_info;;
-d|--directory)_check_longoptions "$1" "$2"
FOLDER="$2"&&shift
;;
-s|--source)_check_longoptions "$1" "$2"
_SOURCE="${2/default=/}"
{ [[ $2 == default* ]]&&UPDATE_DEFAULT_SOURCE="_update_config";}||:
for _source in "${ALL_SOURCES[@]}";do
if [[ "$_source"-scraper.bash == "$_SOURCE"-scraper.bash ]];then
SOURCE="$_SOURCE"&&shift
break
fi
done
if [[ -z $SOURCE ]];then
printf "%s\n" "Error: Given source ( $2 ) is not supported."
exit 1
fi
;;
-n|--num)_check_longoptions "$1" "$2"
_NUM_OF_SEARCH="${2/default=/}"
{ [[ $2 == default* ]]&&UPDATE_DEFAULT_NUM_OF_SEARCH="_update_config";}||:
case "$_NUM_OF_SEARCH" in
all|*[0-9]*)\
NUM_OF_SEARCH="$2"
;;
*)printf "\nError: -n/--num accept arguments as postive integets.\n"
exit 1
esac
shift
;;
-p|--parallel)_check_longoptions "$1" "$2"
NO_OF_PARALLEL_JOBS="$2"
case "$NO_OF_PARALLEL_JOBS" in
''|*[!0-9]*)printf "\nError: -p/--parallel value ranges between 1 to 10.\n"
exit 1
;;
*)[[ $NO_OF_PARALLEL_JOBS -gt 10 ]]&&{
NO_OF_PARALLEL_JOBS=10||NO_OF_PARALLEL_JOBS="$2"
}
esac
PARALLEL_DOWNLOAD="true"&&export PARALLEL_DOWNLOAD
shift
;;
-r|--range)\
MODIFY_RANGE="true" \
RANGE_MODE="relative" \
do_shift=""
for i in $2;do
if [[ $i =~ (^([0-9]+)-([0-9]+|last)+$|^([0-9]+|last)+$) ]];then
RELATIVE_RANGE+=("$i")
do_shift=1
fi
done
[[ -n $do_shift ]]&&shift
;;
-ra|--range-absolute)\
MODIFY_RANGE="true" \
RANGE_MODE="absolute" \
do_shift=""
for i in $2;do
if [[ $i =~ (^([0-9.]+)-([0-9.]+|last)+$|^([0-9.]+|last)+$) ]];then
ABSOLUTE_RANGE+=("$i")
do_shift=1
fi
done
[[ -n $do_shift ]]&&shift
;;
-c|--convert)_check_longoptions "$1" "$2"
DECREASE_QUALITY="$2"
case "$DECREASE_QUALITY" in
''|*[!0-9]*)printf "\nError: -c/--convert value ranges between 1 to 100.\n"
exit 1
;;
*)[[ $DECREASE_QUALITY -gt 99 ]]&&{
DECREASE_QUALITY=99||DECREASE_QUALITY="$2"
}
esac
CONVERT="true"&&CONVERT_DIR="converted"
shift
;;
-z|--zip)CREATE_ZIP="true";;
-U|--upload)UPLOAD_ZIP="true";;
--skip-internet-check)SKIP_INTERNET_CHECK=":";;
'')shorthelp;;
*)if
[[ $1 == -* ]]
then
printf '%s: %s: Unknown option\nTry '"%s -h/--help"' for more information.\n' "${0##*/}" "$1" "${0##*/}"&&exit 1
else
INPUT_ARRAY+=("$1")
fi
esac
shift
done
_check_debug
SOURCE="${SOURCE:-mangafox}"
"${UPDATE_DEFAULT_SOURCE:-:}" SOURCE "$SOURCE" "$CONFIG"
NUM_OF_SEARCH="${NUM_OF_SEARCH:-10}"
"${UPDATE_DEFAULT_NUM_OF_SEARCH:-:}" NUM_OF_SEARCH "$NUM_OF_SEARCH" "$CONFIG"
{ [[ $NUM_OF_SEARCH == all ]]&&unset NUM_OF_SEARCH;}||:
CORES="$({ nproc||sysctl -n hw.logicalcpu;} 2>|/dev/null)"
NO_OF_PARALLEL_JOBS="${NO_OF_PARALLEL_JOBS:-$CORES}"
[[ -z ${INPUT_ARRAY[*]} ]]&&_short_help
export -f _basename _tail _regex
return 0
}
_source_manga_util(){
SOURCE="${1:-$SOURCE}"
case "$SOURCE" in
*mangakakalot*)SOURCE="manganelo";;
*fanfox*)SOURCE="mangafox";;
*gmanga*)for c in jq openssl od
do
command -v "$c" >|/dev/null||{ printf "%s\n" "Install $c to use gmanga"&&return 1;}
done
esac
[[ -z $SELF_SOURCE ]]&&{
utils_file="$UTILS_FOLDER/$SOURCE-scraper.bash"
if [[ -r $utils_file ]];then
. "$utils_file"||{ printf "Error: Unable to source file ( %s ) .\n" "$utils_file" 1>&2&&exit 1;}
else
printf "Error: Utils file ( %s ) not found\n" "$utils_file" 1>&2
exit 1
fi
}
return 0
}
_process_arguments(){
declare input utils_file _exit
_source_manga_util||return 1
CURRENT_DIR="$PWD"
{ [[ -n $FOLDER ]]&&mkdir -p "$FOLDER"&&{ cd "${FOLDER:-.}"||exit 1;};}||:
declare -A Aseen
for input in "${INPUT_ARRAY[@]}";do
{ [[ ${Aseen[$input]} ]]&&continue;}||Aseen[$input]=x
if [[ $input =~ $url_regex ]];then
if [[ $input =~ $source_regex ]];then
source_of_url="$(_regex "$input" ''"$(printf "%s|" "${ALL_SOURCES[@]}")examplemanga"'' 0)"
_source_manga_util "$source_of_url"
_print_center "justify" "Fetching manga details.." "-"
_fetch_manga_details_"$SOURCE" "$input" fetch_name||{ _clear_line 1&&_print_center "justify" "Error: Invalid manga url." "="&&_newline "\n"&&continue;}
_clear_line 1
_print_center "justify" "$NAME" "="
else
_print_center "justify" "URL not supported." "="
_newline "\n"&&continue
fi
else
unset _exit option RANGE _option _RANGE
_print_center "justify" "$input" "="
_print_center "justify" "Searching in" " $SOURCE" "-"
_search_manga_"$SOURCE" "$input" "$NUM_OF_SEARCH"||_exit="1"
_clear_line 1
_print_center "justify" "Source" ": $SOURCE" "="
_print_center "justify" "${TOTAL_SEARCHES:-0} results found" "="
{ [[ -n $_exit ]]&&_newline "\n"&&continue;}||:
printf "\n%s\n" "${OPTION_NAMES[@]}"&&_newline "\n"
"${QUIET:-_print_center}" "normal" " Choose " "-"
until [[ $option =~ ^([0-9]+)+$ && $option -gt 0 && $option -le ${#OPTION_NAMES[@]} ]];do
[[ -n $_option ]]&&_clear_line 1
printf -- "-> "
read -r option&&_option=1
done
_set_manga_variables_"$SOURCE" "$option"
_print_center "justify" "$NAME" "="
_print_center "justify" "Fetching manga details.." "-"
_fetch_manga_details_"$SOURCE" "${URL:-$SLUG}"
_clear_line 1
fi
export SOURCE
mkdir -p "$NAME"
FULL_PATH_NAME="$(printf "%s/%s\n" "$(cd "$(_dirname "$NAME")" &>/dev/null&&pwd)" "${NAME##*/}")"
cd "$NAME"||exit 1
FINAL_RANGE="${PAGES[0]}"-"${PAGES[$((${#PAGES[@]}-1))]}"
if [[ -n $MODIFY_RANGE ]];then
if [[ -z ${RELATIVE_RANGE[*]:-${ABSOLUTE_RANGE[*]}} ]];then
_print_center "justify" "Input chapters" "-"
printf "%b " "${PAGES[@]}"&&_newline "\n\n"
"${QUIET:-_print_center}" "normal" " Give range, e.g: 1 2-10 69 " "-"
until [[ -n ${GIVEN_RANGE[*]} ]];do
[[ -n $_GIVEN_RANGE ]]&&_clear_line 1
printf -- "-> "
read -ra GIVEN_RANGE&&_GIVEN_RANGE=1
done
_check_and_create_range "$RANGE_MODE" "${GIVEN_RANGE[@]}"||continue
fi
[[ -n ${RELATIVE_RANGE[*]} ]]&&{ _check_and_create_range relative "${RELATIVE_RANGE[@]}"||continue;}
[[ -n ${ABSOLUTE_RANGE[*]} ]]&&{ _check_and_create_range absolute "${ABSOLUTE_RANGE[@]}"||continue;}
mapfile -t PAGES <<<"$(printf "%s\n" "${PAGES[@]}"|sed -e "s/^/_-_-_/g" -e "s/$/_-_-_/g")"
for _range in "${RANGE[@]}";do
regex=""
if [[ $_range == *-* ]];then
regex="_-_-_${_range/-*/}_-_-_.*_-_-_${_range/*-/}_-_-_"
[[ ${PAGES[*]} =~ $regex ]]&&TEMP_PAGES+="$(: "${BASH_REMATCH[@]//_-_-_ _-_-_/$'\n'}"&&printf "%s\n" "${_//_-_-_/}")
"
else
regex="_-_-_${_range}_-_-_"
[[ ${PAGES[*]} =~ $regex ]]&&TEMP_PAGES+="$(: "${BASH_REMATCH[@]//_-_-_ _-_-_/$'\n'}"&&printf "%s\n" "${_//_-_-_/}")
"
fi
done
mapfile -t PAGES <<<"$TEMP_PAGES"
mapfile -t PAGES <<<"$(_remove_array_duplicates "${PAGES[@]}")"
FINAL_RANGE="$(: "$(_remove_array_duplicates "${RANGE[@]}")"&&printf "%s\n" "${_//$'\n'/,}")"
fi
TOTAL_CHAPTERS="${#PAGES[@]}"
_print_center "justify" "$TOTAL_CHAPTERS chapter(s)" "="
_print_center "justify" "Fetching" " chapter details.." "-"
mkdir -p "${PAGES[@]}"
_fetch_manga_chapters
_clear_line 1
_print_center "justify" "Counting Images.." "-"
_count_images_"$SOURCE"
for _ in {1..2};do _clear_line 1;done
_print_center "justify" "$TOTAL_CHAPTERS chapters" " | $TOTAL_IMAGES images" "="
_print_center "justify" "Downloading" " images.." "-"&&_newline "\n"
_download_images
_clear_line 1
TOTAL_IMAGES="${#IMAGES[@]}"
_print_center "justify" "$TOTAL_IMAGES" " images downloaded." "="
_print_center "justify" "$TOTAL_IMAGES_SIZE" "="&&_newline "\n"
if [[ -n $CONVERT ]];then
if command -v convert 1>/dev/null;then
_print_center "justify" "Converting chapters .." "-"
_print_center "justify" "Quality to decrease: $DECREASE_QUALITY%" "="&&_newline "\n"
export DECREASE_QUALITY CONVERT_DIR
{ mkdir -p "$CONVERT_DIR"&&cd "$CONVERT_DIR"&&mkdir -p "${PAGES[@]}"&&cd - &>/dev/null;}||exit 1
_convert_page(){
declare page="${1:?}" copy images image current_quality new_quality
mapfile -t images <<<"$(printf "%b\n%b\n" "$page/"*jpg "$page/"*png "$page/"*webp|grep -vE '\*png|\*jpg|\*webp')"
image="${images[0]}"
current_quality="$(identify -format %Q "$image")"
new_quality="$((DECREASE_QUALITY<current_quality?(current_quality-DECREASE_QUALITY):current_quality))"
rm -f "$CONVERT_DIR/$page/"*
if [[ $new_quality -lt $current_quality ]];then
mogrify -format jpg -path "$CONVERT_DIR/$page" -quality "$new_quality" "${images[@]}" &>/dev/null&&{ printf "1\n"||copy=1;}
elif [[ $image =~ png ]];then
mogrify -format jpg -path "$CONVERT_DIR/$page" "${images[@]}" &>/dev/null&&{ printf "1\n"||copy=1;}
else
copy=1
fi
[[ -n $copy ]]&&{
printf "2\n" 1>&2
cp -u "${images[@]}" "$CONVERT_DIR/$page/"
}
}
export -f _head _convert_page
printf "%s\n" "${PAGES[@]}"|xargs -P "${NO_OF_PARALLEL_JOBS:-$((CORES*2))}" -n 1 -I "{}" bash -c \
'_convert_page "{}"' 1>"$TMPFILE".success 2>"$TMPFILE".error&
pid="$!"
until [[ -f "$TMPFILE".success || -f "$TMPFILE".error ]];do sleep 0.5;done
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
SUCCESS_STATUS="$(_count <"$TMPFILE".success)"
ERROR_STATUS="$(_count <"$TMPFILE".error)"
sleep 1
if [[ $TOTAL_STATUS != "$((SUCCESS_STATUS+ERROR_STATUS))" ]];then
_clear_line 1
_print_center "justify" "$SUCCESS_STATUS converted" " | $ERROR_STATUS copied" "="
fi
TOTAL_STATUS="$((SUCCESS_STATUS+ERROR_STATUS))"
done
rm -f "$TMPFILE".success "$TMPFILE".error
for _ in {1..3};do _clear_line 1;done
_print_center "justify" "Converted $TOTAL_IMAGES" " images ( $DECREASE_QUALITY% )" "="
else
_print_center "justify" "Imagemagick not installed, skipping conversion.." "="
unset CONVERT_DIR
fi
fi
if [[ -n $CREATE_ZIP ]];then
if command -v zip 1>/dev/null;then
_print_center "justify" "Creating zip.." "="&&_newline "\n"
cd "${CONVERT_DIR:-.}"||exit
ZIPNAME="$NAME${FINAL_RANGE+_$FINAL_RANGE}${CONVERT_DIR+_decreased_$DECREASE_QUALITY}".zip
zip -x "*chapter" "*images" -u -q -r9 -lf "$TMPFILE".log -li -la "$FULL_PATH_NAME"/"$ZIPNAME" "${PAGES[@]}" &>/dev/null&
pid="$!"
until [[ -f "$TMPFILE".log ]];do sleep 0.5;done
TOTAL_ZIP_STATUS="$((TOTAL_IMAGES+TOTAL_CHAPTERS))"
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
STATUS=$(grep 'up to date\|updating\|adding' "$TMPFILE".log -c)
sleep 0.5
if [[ $STATUS != "$OLD_STATUS" ]];then
_clear_line 1
_print_center "justify" "$STATUS" " / $TOTAL_ZIP_STATUS" "="
fi
OLD_STATUS="$STATUS"
done
for _ in {1..2};do _clear_line 1;done
rm -f "$TMPFILE".log
ZIP_SIZE="$(_bytes_to_human "$(wc -c <"$FULL_PATH_NAME/$ZIPNAME")")"
_print_center "justify" "$ZIPNAME" "="
_newline "\n"&&_print_center "normal" "Path: \"${FULL_PATH_NAME/$CURRENT_DIR\//}/$ZIPNAME\" " " "&&_newline "\n"
_print_center "justify" "$ZIP_SIZE" "="&&_newline "\n"
if [[ -n $UPLOAD_ZIP ]];then
_print_center "justify" "Uploading zip.." "-"
DOWNLOAD_URL="$(upload "$ZIPNAME")"
for _ in {1..2};do _clear_line 1;done
_print_center "justify" "ZipLink" "="
_print_center "normal" "$(printf "%b\n" "\xe2\x86\x93 \xe2\x86\x93 \xe2\x86\x93")" " "
_print_center "normal" "$DOWNLOAD_URL" " "&&_newline "\n"
fi
else
_print_center "justify" "zip not installed, skipping zip creation.." "="
fi
fi
done
return 0
}
main(){
[[ $# == 0 ]]&&_short_help
[[ -z $SELF_SOURCE ]]&&{
UTILS_FOLDER="${UTILS_FOLDER:-${INSTALL_PATH:-./utils}}"
{ . "$UTILS_FOLDER"/common-utils.bash&&. "$UTILS_FOLDER"/scraper-utils.bash;}||{ printf "Error: Unable to source util files.\n"&&exit 1;}
for _source in "$UTILS_FOLDER"/*scraper.bash;do
ALL_SOURCES+=("$(_basename "${_source/-scraper.bash/}")")
done
}
_check_bash_version&&set -o errexit -o noclobber -o pipefail
_setup_arguments "$@"
"${SKIP_INTERNET_CHECK:-_check_internet}"
[[ -n $PARALLEL_DOWNLOAD ]]&&{
{ command -v mktemp 1>|/dev/null&&TMPFILE="$(mktemp -u)";}||TMPFILE="$PWD/$(printf '%(%s)T\n' "-1").LOG"
}
_cleanup(){
{
script_children_pids="$(ps --ppid="$MAIN_PID" -o pid=)"
kill $script_children_pids 1>|/dev/null
[[ -n $PARALLEL_DOWNLOAD ]]&&rm -f "${TMPFILE:?}"*
export abnormal_exit&&if [[ -n $abnormal_exit ]];then
printf "\n\n%s\n" "Script exited manually."
kill -- -$$&
else
_auto_update
fi
} 2>|/dev/null||:
return 0
}
trap 'abnormal_exit="1"; exit' INT TERM
trap '_cleanup' EXIT
trap '' TSTP
START="$(printf '%(%s)T\n' "-1")"
_process_arguments
END="$(printf '%(%s)T\n' "-1")"
DIFF="$((END-START))"
"${QUIET:-_print_center}" "normal" " Time Elapsed: ""$((DIFF/60))"" minute(s) and ""$((DIFF%60))"" seconds " "="
}
{ [[ -z ${SOURCED_MANGADL:-} ]]&&main "$@";}||:
